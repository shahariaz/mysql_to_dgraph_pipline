# MySQL to Dgraph Migration Pipeline

A production-ready Go application that migrates data from MySQL databases to Dgraph, preserving foreign key relationships and generating optimized schemas.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MySQL DB     â”‚â”€â”€â”€â–¶â”‚  Pipeline Core   â”‚â”€â”€â”€â–¶â”‚   Dgraph DB     â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Schema        â”‚    â”‚ â€¢ Schema Extract â”‚    â”‚ â€¢ RDF Data      â”‚
â”‚ â€¢ Data          â”‚    â”‚ â€¢ Data Process   â”‚    â”‚ â€¢ Graph Schema  â”‚
â”‚ â€¢ Relationships â”‚    â”‚ â€¢ Validation     â”‚    â”‚ â€¢ Relationships â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
mysql_to_dgraph_pipeline/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                 # Application entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.go          # Configuration management
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ pipeline.go        # Core pipeline orchestration
â”‚       â”œâ”€â”€ schema.go          # MySQL schema extraction
â”‚       â”œâ”€â”€ processor.go       # Data processing & conversion
â”‚       â”œâ”€â”€ generator.go       # Dgraph schema generation
â”‚       â”œâ”€â”€ validator.go       # Data validation
â”‚       â””â”€â”€ chunked_exporter.go # Chunked data export
â”œâ”€â”€ pkg/
â”‚   â””â”€â”€ logger/
â”‚       â””â”€â”€ logger.go          # Structured logging
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml           # Default configuration
â”‚   â”œâ”€â”€ production.yaml       # Production settings
â”‚   â””â”€â”€ large_scale_test.yaml # Large scale test config
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh              # Environment setup
â”œâ”€â”€ output/                   # Generated files directory
â”œâ”€â”€ complete_pipeline.sh      # Full automation script
â”œâ”€â”€ quick_import.sh           # Quick import script
â””â”€â”€ docker-compose.yaml       # Dgraph containerization
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Go 1.19+
- MySQL 5.7+ or 8.0+
- Docker & Docker Compose (for Dgraph)

### 2. Setup Environment

```bash
# Clone and setup
git clone <repository>
cd mysql_to_dgraph_pipeline

# Start Dgraph
docker-compose up -d

# Configure database connection
cp config/config.yaml config/local.yaml
# Edit config/local.yaml with your MySQL credentials
```

### 3. Run Migration

```bash
# Full pipeline (recommended)
go run cmd/main.go -config config/local.yaml -mode full

# Or use automation script
./complete_pipeline.sh
```

## âš™ï¸ Configuration

### Configuration File (YAML)

```yaml
mysql:
  host: "localhost"
  port: 3306
  user: "root"
  password: "password"
  database: "your_database"
  max_connections: 10
  timeout: "30s"

dgraph:
  alpha: ["localhost:9080"]
  timeout: "30s"
  batch_size: 10000

pipeline:
  workers: 4
  batch_size: 1000
  memory_limit_mb: 1024
  dry_run: false
  skip_validation: false

logger:
  level: "info"
  format: "json"

output:
  directory: "output"
  rdf_file: "data.rdf"
  schema_file: "schema.txt"
  mapping_file: "uid_mapping.json"
```

### Environment Variables

Override any configuration with environment variables:

```bash
export MYSQL_HOST=localhost
export MYSQL_DATABASE=your_db
export PIPELINE_WORKERS=8
export LOG_LEVEL=debug
```

## ğŸ”§ Usage Modes

### Schema Only
Extract MySQL schema and generate Dgraph schema:
```bash
go run cmd/main.go -mode schema
```

### Data Only
Migrate data using existing schema:
```bash
go run cmd/main.go -mode data
```

### Full Pipeline
Complete migration with validation:
```bash
go run cmd/main.go -mode full
```

### Validation Only
Validate existing migration:
```bash
go run cmd/main.go -mode validate
```

### Advanced Options

```bash
# Process specific tables
go run cmd/main.go -tables "users,products,orders"

# Increase parallelism
go run cmd/main.go -parallel 8 -batch-size 2000

# Dry run (preview without changes)
go run cmd/main.go -dry-run
```

## ğŸ“Š Components Deep Dive

### 1. Schema Extractor (`internal/pipeline/schema.go`)
- **Purpose**: Analyzes MySQL database structure
- **Features**: 
  - Table and column metadata extraction
  - Foreign key relationship discovery
  - Index and constraint analysis
  - Convention-based FK detection

### 2. Data Processor (`internal/pipeline/processor.go`)
- **Purpose**: Converts MySQL data to RDF format
- **Features**:
  - Parallel processing with worker pools
  - Memory-efficient streaming
  - Progress tracking and reporting
  - Error handling and recovery

### 3. Schema Generator (`internal/pipeline/generator.go`)
- **Purpose**: Creates Dgraph schema from MySQL structure
- **Features**:
  - Type mapping (MySQL â†’ Dgraph)
  - Relationship preservation
  - Index optimization
  - Custom type handling

### 4. Data Validator (`internal/pipeline/validator.go`)
- **Purpose**: Ensures migration accuracy and integrity
- **Features**:
  - Row count validation
  - Foreign key integrity checks
  - Data type consistency
  - File integrity verification

### 5. Logger (`pkg/logger/logger.go`)
- **Purpose**: Structured logging throughout the pipeline
- **Features**:
  - Multiple output formats (JSON/Text)
  - Configurable log levels
  - Contextual field logging
  - Performance monitoring

## ğŸ” Output Files

### Generated Files

1. **`output/data.rdf`**: Complete dataset in RDF format
2. **`output/schema.txt`**: Dgraph schema definitions
3. **`output/uid_mapping.json`**: UID mappings for reference
4. **`output/checkpoint.json`**: Progress checkpoints

### Import to Dgraph

```bash
# Copy files to Dgraph container
./quick_import.sh

# Or manual import
cat output/data.rdf | docker exec -i dgraph-alpha tee /tmp/data.rdf
docker exec dgraph-alpha dgraph live --rdfs=/tmp/data.rdf --alpha=localhost:9080
```

## ğŸ› Troubleshooting

### Common Issues

1. **Connection Errors**
   ```bash
   # Check MySQL connectivity
   mysql -h localhost -u root -p your_database
   
   # Check Dgraph status
   curl http://localhost:8080/health
   ```

2. **Memory Issues**
   ```bash
   # Reduce batch size
   go run cmd/main.go -batch-size 500
   
   # Increase memory limit in config
   memory_limit_mb: 2048
   ```

3. **Permission Errors**
   ```bash
   # Ensure output directory exists and is writable
   mkdir -p output
   chmod 755 output
   ```

### Debug Mode

Enable detailed logging:
```bash
go run cmd/main.go -config config/local.yaml -mode full
# Set LOG_LEVEL=debug in config or environment
```

## ğŸ“ˆ Performance Tuning

### Database Optimization

```sql
-- MySQL optimizations
SET SESSION sql_mode = 'TRADITIONAL';
SET SESSION tx_isolation = 'READ-COMMITTED';
```

### Pipeline Tuning

```yaml
pipeline:
  workers: 8              # CPU cores * 2
  batch_size: 2000        # Higher for better throughput
  memory_limit_mb: 4096   # Based on available RAM
```

### Dgraph Optimization

```yaml
dgraph:
  batch_size: 20000       # Larger batches for bulk import
  compression: true       # Enable for network efficiency
```

## ğŸ”’ Security Considerations

1. **Database Credentials**: Use environment variables or secure config files
2. **Network Security**: Ensure proper firewall rules for database access
3. **Data Privacy**: Consider data masking for sensitive information
4. **Access Control**: Implement proper authentication for Dgraph

## ğŸ“š API Reference

### Pipeline Interface

```go
type Pipeline interface {
    ExtractSchema() error
    MigrateData(tables string) error
    GenerateDgraphSchema() error
    ValidateData() error
    RunFull(tables string) error
    Stop()
}
```

### Configuration Interface

```go
type Config interface {
    Load(configPath string) (*Config, error)
    Validate() error
    ConnectionString() string
}
```

## ğŸ¤ Contributing

1. Follow Go conventions and best practices
2. Add comprehensive tests for new features
3. Update documentation for API changes
4. Use structured logging throughout
5. Ensure backward compatibility

## ğŸ“„ License

[Your License Here]

---

For more detailed examples and advanced usage, see:
- `PIPELINE_COMMANDS.md` - Complete command reference
- `LARGE_SCALE_TESTING.md` - Performance testing guide
- `QUICKSTART.md` - Quick setup guide

## ğŸ§¹ Codebase Cleanup Summary

This codebase has been thoroughly cleaned and refactored for production use:

### Files Removed:
- `test/` directory - Removed legacy test artifacts
- Multiple shell scripts - Consolidated into `scripts/setup.sh`
- Build artifacts and temporary files
- Redundant helper functions and unused imports

### Code Improvements:
- **Comprehensive Comments**: All major functions and structs now have detailed documentation
- **Consistent Structure**: Standardized code organization across all modules
- **Error Handling**: Improved error messages and handling throughout
- **Performance**: Optimized memory usage and processing efficiency
- **Maintainability**: Clear separation of concerns and modular design

### Quality Assurance:
- âœ… No compilation errors
- âœ… No unused functions or imports
- âœ… Consistent naming conventions
- âœ… Proper documentation
- âœ… Clean git history

### Next Steps for Development:
1. Run `./scripts/setup.sh` for initial setup
2. Configure your MySQL connection in `.env`
3. Use the pipeline with: `./pipeline -config config/config.yaml -mode full`
4. Monitor logs for performance and debugging information

The codebase is now production-ready and fully documented for easy maintenance and future development.
